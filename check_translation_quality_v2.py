#!/usr/bin/env python3
"""
ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…· V2
æ›´æ™ºèƒ½çš„è¯­è¨€æ£€æµ‹ï¼Œå‡å°‘è¯¯æŠ¥
"""

import json
import os
import re
from collections import defaultdict
import argparse


def load_json_file(filepath):
    """åŠ è½½JSONæ–‡ä»¶"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def contains_english_keywords(text, lang_code):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«è‹±æ–‡å…³é”®è¯ï¼ˆæ›´æ™ºèƒ½ï¼‰"""
    # å¸¸è§çš„è‹±æ–‡UIè¯æ±‡
    keywords = [
        'Please', 'Enter', 'Select', 'Password', 'Login',
        'Settings', 'Edit', 'Delete', 'Clear', 'Copy',
        'Download', 'Upload', 'Cancel', 'Confirm', 'Save',
        'verification', 'progress', 'merchant', 'payment',
        'Withdrawal', 'Payment', 'Error', 'Success'
    ]
    
    # è¯­è¨€ç‰¹å®šçš„è±å…è¯æ±‡ï¼ˆè¿™äº›è¯åœ¨è¯¥è¯­è¨€ä¸­æ˜¯æ­£ç¡®çš„ï¼‰
    language_exemptions = {
        'es': ['confirmar', 'cancelar', 'editar', 'error'],
        'pt': ['confirmar', 'cancelar', 'editar', 'upload', 'login', 'download'],
        'fr': ['confirmer', 'entrer', 'enter', 'error'],
        'it': ['password', 'login', 'cancella', 'download', 'upload', 'error'],
        'de': ['download', 'upload', 'center', 'service', 'error']
    }
    
    # å…¨å±€è±å…è¯æ±‡ï¼ˆå“ç‰Œåã€æŠ€æœ¯æœ¯è¯­ç­‰ï¼‰
    global_exemptions = ['APP', 'iOS', 'Android', 'ID', 'VIP', 'API', 'URL', 'T+1', 
                        'USDT', 'H5', 'PC', 'OK', 'PDF', 'HTML', 'JSON']
    
    text_lower = text.lower()
    exemptions = language_exemptions.get(lang_code, [])
    
    for keyword in keywords:
        if keyword.lower() in text_lower:
            # æ£€æŸ¥æ˜¯å¦åœ¨è±å…åˆ—è¡¨ä¸­
            is_exempted = False
            
            # æ£€æŸ¥è¯­è¨€ç‰¹å®šè±å…
            for exemption in exemptions:
                if exemption in text_lower:
                    is_exempted = True
                    break
            
            # æ£€æŸ¥å…¨å±€è±å…
            for exemption in global_exemptions:
                if exemption.lower() in text_lower:
                    is_exempted = True
                    break
            
            if not is_exempted:
                return True, keyword
    
    return False, None


def check_english_in_non_english_file(data, language_code):
    """æ£€æŸ¥éè‹±æ–‡æ–‡ä»¶ä¸­çš„è‹±æ–‡å†…å®¹"""
    if language_code == 'en':
        return []
    
    issues = []
    
    for key, value in data.items():
        if isinstance(value, str):
            has_english, keyword = contains_english_keywords(value, language_code)
            if has_english:
                issues.append({
                    'key': key,
                    'value': value,
                    'issue': f'åŒ…å«è‹±æ–‡è¯æ±‡: {keyword}'
                })
    
    return issues


def check_capitalization_issues(data, language_code):
    """æ£€æŸ¥å¤§å†™é—®é¢˜"""
    issues = []
    
    # ç½—æ›¼è¯­ç³»è¯­è¨€åº”è¯¥ä½¿ç”¨å°å†™
    lowercase_languages = ['es', 'fr', 'it', 'pt']
    
    if language_code in lowercase_languages:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å½“çš„å¤§å†™
        for key, value in data.items():
            if isinstance(value, str) and len(value) > 0:
                # è·³è¿‡ç¼©å†™è¯å’Œå“ç‰Œå
                if value.isupper() or len(value) <= 3:
                    continue
                
                # è·³è¿‡åŒ…å«ç‰¹æ®Šè¯æ±‡çš„æ¡ç›®
                skip_words = ['VIP', 'APP', 'USDT', 'ID', 'PC', 'H5', 'API']
                should_skip = any(word in value for word in skip_words)
                if should_skip:
                    continue
                
                # æ£€æŸ¥é¦–å­—æ¯å¤§å†™çš„å•è¯ï¼ˆä¸åœ¨å¥é¦–ï¼‰
                words = value.split()
                if len(words) == 1 and words[0][0].isupper() and words[0][1:].islower():
                    # å•ä¸ªè¯ä¸”é¦–å­—æ¯å¤§å†™
                    issues.append({
                        'key': key,
                        'value': value,
                        'issue': f'{language_code}è¯­è¨€UIå…ƒç´ åº”è¯¥ä½¿ç”¨å°å†™'
                    })
    
    return issues


def check_translation_consistency(data, reference_keys):
    """æ£€æŸ¥ç¿»è¯‘å®Œæ•´æ€§"""
    missing_keys = []
    
    for key in reference_keys:
        if key not in data:
            missing_keys.append(key)
    
    return missing_keys


def analyze_language_file(filepath, reference_data=None):
    """åˆ†æå•ä¸ªè¯­è¨€æ–‡ä»¶"""
    filename = os.path.basename(filepath)
    language_code = filename.replace('.json', '')
    
    print(f"\n{'='*60}")
    print(f"åˆ†ææ–‡ä»¶: {filename}")
    print(f"{'='*60}")
    
    try:
        data = load_json_file(filepath)
        
        # 1. æ£€æŸ¥è‹±æ–‡æ··å…¥
        english_issues = check_english_in_non_english_file(data, language_code)
        if english_issues:
            print(f"\nâŒ å‘ç° {len(english_issues)} ä¸ªè‹±æ–‡æ··å…¥é—®é¢˜:")
            for issue in english_issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {issue['key']}: \"{issue['value']}\" ({issue['issue']})")
            if len(english_issues) > 10:
                print(f"  ... è¿˜æœ‰ {len(english_issues) - 10} ä¸ªé—®é¢˜")
        
        # 2. æ£€æŸ¥å¤§å†™é—®é¢˜
        cap_issues = check_capitalization_issues(data, language_code)
        if cap_issues:
            print(f"\nâš ï¸  å‘ç° {len(cap_issues)} ä¸ªå¤§å†™é—®é¢˜:")
            for issue in cap_issues[:5]:
                print(f"  - {issue['key']}: \"{issue['value']}\" ({issue['issue']})")
            if len(cap_issues) > 5:
                print(f"  ... è¿˜æœ‰ {len(cap_issues) - 5} ä¸ªé—®é¢˜")
        
        # 3. æ£€æŸ¥ç¼ºå¤±çš„é”®
        if reference_data:
            missing_keys = check_translation_consistency(data, reference_data.keys())
            if missing_keys:
                print(f"\nâš ï¸  ç¼ºå¤± {len(missing_keys)} ä¸ªç¿»è¯‘é”®:")
                for key in missing_keys[:5]:
                    print(f"  - {key}")
                if len(missing_keys) > 5:
                    print(f"  ... è¿˜æœ‰ {len(missing_keys) - 5} ä¸ªç¼ºå¤±")
        
        # 4. ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ€»æ¡ç›®æ•°: {len(data)}")
        print(f"  - è‹±æ–‡æ··å…¥ç‡: {len(english_issues)/len(data)*100:.1f}%")
        
        if not english_issues and not cap_issues and not (reference_data and missing_keys):
            print(f"\nâœ… æ–‡ä»¶è´¨é‡è‰¯å¥½!")
        
        return {
            'language': language_code,
            'total_entries': len(data),
            'english_issues': len(english_issues),
            'cap_issues': len(cap_issues),
            'missing_keys': len(missing_keys) if reference_data else 0
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None


def check_directory(directory_path):
    """æ£€æŸ¥ç›®å½•ä¸­çš„æ‰€æœ‰è¯­è¨€æ–‡ä»¶"""
    print(f"æ£€æŸ¥ç›®å½•: {directory_path}")
    
    # é¦–å…ˆåŠ è½½å‚è€ƒæ–‡ä»¶ï¼ˆä¸­æ–‡ï¼‰
    reference_file = os.path.join(directory_path, 'zh-CN.json')
    reference_data = None
    
    if os.path.exists(reference_file):
        reference_data = load_json_file(reference_file)
        print(f"ä½¿ç”¨ zh-CN.json ä½œä¸ºå‚è€ƒ ({len(reference_data)} ä¸ªé”®)")
    
    # åˆ†ææ‰€æœ‰JSONæ–‡ä»¶
    results = []
    for filename in sorted(os.listdir(directory_path)):
        if filename.endswith('.json') and filename != 'zh-CN.json':
            filepath = os.path.join(directory_path, filename)
            result = analyze_language_file(filepath, reference_data)
            if result:
                results.append(result)
    
    # æ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("æ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*60}")
    
    total_issues = sum(r['english_issues'] + r['cap_issues'] + r['missing_keys'] for r in results)
    
    if total_issues == 0:
        print("âœ… æ‰€æœ‰æ–‡ä»¶è´¨é‡è‰¯å¥½!")
    else:
        print(f"âš ï¸  å‘ç°æ€»è®¡ {total_issues} ä¸ªé—®é¢˜")
        print("\né—®é¢˜åˆ†å¸ƒ:")
        
        # æŒ‰é—®é¢˜æ•°é‡æ’åº
        sorted_results = sorted(results, 
                              key=lambda x: x['english_issues'] + x['cap_issues'] + x['missing_keys'], 
                              reverse=True)
        
        for i, result in enumerate(sorted_results[:10]):
            total = result['english_issues'] + result['cap_issues'] + result['missing_keys']
            if total > 0:
                parts = []
                if result['english_issues'] > 0:
                    parts.append(f"è‹±æ–‡æ··å…¥: {result['english_issues']}")
                if result['cap_issues'] > 0:
                    parts.append(f"å¤§å†™é—®é¢˜: {result['cap_issues']}")
                if result['missing_keys'] > 0:
                    parts.append(f"ç¼ºå¤±ç¿»è¯‘: {result['missing_keys']}")
                
                print(f"{i+1}. {result['language']}.json - {total} ä¸ªé—®é¢˜ ({', '.join(parts)})")
        
        # æ€»ç»“
        print(f"\nğŸ“Š è´¨é‡æ€»ç»“:")
        excellent = [r for r in results if r['english_issues'] == 0 and r['cap_issues'] == 0 and r['missing_keys'] == 0]
        good = [r for r in results if r['english_issues'] < 5 and r['cap_issues'] < 5 and r['missing_keys'] < 10]
        
        if excellent:
            print(f"  - ä¼˜ç§€: {', '.join([r['language'] + '.json' for r in excellent])}")
        if good and len(good) != len(excellent):
            print(f"  - è‰¯å¥½: {', '.join([r['language'] + '.json' for r in good if r not in excellent])}")


def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥ç¿»è¯‘æ–‡ä»¶è´¨é‡ V2')
    parser.add_argument('path', help='è¦æ£€æŸ¥çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--strict', action='store_true', help='ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼')
    
    args = parser.parse_args()
    
    if os.path.isdir(args.path):
        check_directory(args.path)
    elif os.path.isfile(args.path):
        analyze_language_file(args.path)
    else:
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {args.path}")


if __name__ == "__main__":
    main()